# 예상 질문 & 답변

회원·전시 도메인 정리 문서를 바탕으로 한 **면접 예상 질문**과 **답변**입니다. 말할 때는 1~2분 안에 핵심만 전달하도록 요약해서 사용하세요.

---

## 1. 아키텍처·전체 구조

### Q1. 우리 시스템 전체 아키텍처를 한 줄로 설명해 주세요.

회원은 레거시(WAR)와 MSA를 함께 쓰는 **하이브리드**이고, 전시는 **MSA 위주**로 구성됩니다. 클라이언트는 **Cloud API(BFF)** 하나만 호출하고, BFF가 회원(member/*), 전시(dp/*), 주문·결제 등 도메인별 백엔드로 라우팅하는 구조입니다.

---

### Q2. BFF(Cloud API)를 두는 이유와 장점을 설명해 주세요.

클라이언트가 여러 백엔드 주소를 알 필요 없이 **단일 진입점**만 알면 되게 하려고 BFF를 뒀습니다. 라우팅·인증·버전 관리·응답 조합을 BFF에서 처리해서, 백엔드 서비스를 바꾸거나 추가해도 클라이언트 변경을 최소화할 수 있습니다. 회원이 LGC와 MSA로 나뉘어 있어도 BFF가 둘 다 라우팅해 주기 때문에 **점진적 MSA 전환**이 가능했습니다.

---

### Q3. 회원 도메인에서 레거시와 MSA를 같이 쓰는 이유는 무엇인가요?

한 번에 모놀리스를 없애면 리스크가 커서, **점진적 전환**을 선택했습니다. 기존 LGC는 그대로 두고, 새 기능이나 부하가 큰 구간부터 MSA로 분리해 나갑니다. BFF가 LGC와 MSA를 모두 라우팅하므로 트래픽을 단계적으로 MSA로 옮길 수 있고, 문제가 생기면 라우팅만 되돌리는 식으로 대응할 수 있습니다. 다만 LGC와 MSA가 동시에 있어 **복잡도**는 올라가서, 어떤 API가 어디로 가는지 **라우팅 문서화**를 잘 해두는 게 중요하다고 생각합니다.

---

## 2. 회원 도메인 (MMBR)

### Q4. 회원/멤버십을 query와 command로 나눈 이유를 설명해 주세요.

**CQRS**를 적용해서 조회와 변경을 서비스로 분리했습니다. 조회 트래픽이 훨씬 많기 때문에 query 서비스만 **독립적으로 스케일 아웃**할 수 있고, 쓰기 모델과 읽기 모델을 나눠서 각각에 맞게 최적화할 수 있습니다. 회원·멤버십 둘 다 query/command로 나눠서 **도메인 경계**를 명확히 했습니다.

---

### Q5. 배치를 온라인 서버와 분리한 이유는 무엇인가요?

등급 산정, 쿠폰 적재, 어뷰징 알림, 멤버십 동기화 같은 **대량 작업**을 온라인 트랜잭션 서버에서 돌리면 CPU·메모리·DB 연결을 같이 쓰게 되어 **리소스 경합**이 생깁니다. 배치는 Azkaban으로 스케줄만 걸고, 실제 Job은 **전용 배치 앱**에서만 실행해서 온라인과 리소스와 장애를 분리했습니다. 배치 결과는 Kafka로 넘기고, 필요한 쪽에서 컨슈머로 구독하는 **이벤트 드리븐** 구조로 서비스 간 결합도도 낮췄습니다.

---

### Q6. 회원 FO 로그인은 세션을 어떻게 처리하나요? 수평 확장 시 이슈는 없나요?

FO는 **쿠키와 서버 세션**으로 로그인 상태를 유지합니다. MbrSessionService, SsgCookieHandler로 사용자 정보를 세션에 넣고 이후 요청마다 검증합니다. 세션이 **서버 메모리**에만 있으면 인스턴스를 늘릴 때 **세션 불일치**가 날 수 있어서, 설계 개선으로는 **Redis에 세션을 두고** LGC 웹앱을 무상태로 만드는 방식을 고려하고 있습니다. Redis를 쓰면 인스턴스 추가만으로 수평 확장이 가능하고, 세션 기반이지만 스케일 아웃이 가능하다고 설명할 수 있습니다.

---

### Q7. 회원 도메인에서 아쉬운 점이나 한계를 말한다면?

세 가지를 꼽겠습니다. 첫째, **LGC와 MSA 이원화** 때문에 어떤 API가 어디로 가는지 파악이 어렵고, 라우팅·트래픽 분배 정책 문서가 있으면 좋겠습니다. 둘째, **DB가 하나**라서 장애 시 회원 도메인 전체에 영향이 갈 수 있고, 중장기에는 회원 전용 DB/스키마 분리나 CDC 동기화 같은 설계를 검토하면 좋겠습니다. 셋째, 배치가 **MMBR-LGC 배치**와 **MMBR-MSA 배치** 둘 다 있어서, Job과 실행 앱 매핑을 명확히 하고, 가능하면 한 플랫폼으로 모으면 유지보수에 유리할 것 같습니다.

---

## 3. 전시 도메인 (DP)

### Q8. 전시 도메인은 회원과 어떻게 구분되어 있나요?

**프로젝트와 진입 경로**로 구분합니다. 회원은 MMBR-LGC, MMBR-MSA만 쓰고, 전시는 **MSA의 ssg-dp.*** 와 **FRONT의 ssg-dp.front.api-webapp**만 씁니다. API는 **/member/*** 와 **/dp/api** 등으로 나뉘어 있어서, 채널 웹앱이 회원·전시를 도메인별로 나눠 호출할 수 있습니다. 전시는 처음부터 MSA로 설계되어 레거시 WAR가 없고, Spring Boot·Gradle로 통일되어 있습니다.

---

### Q9. 전시에서 Elasticsearch를 쓰는 용도와 설계 이유를 설명해 주세요.

전시는 **검색과 로그** 용도로 Elasticsearch를 씁니다. 카테고리 데이터는 배치나 컨슈머가 ES에 인덱싱해 두고, 온라인은 **ES만 조회**해서 RDB 부하를 줄이고 검색 성능을 확보합니다. API 접근 로그·트리립 로그는 Kafka 등으로 들어온 걸 **ssg-dp.consumer**가 구독해서 ES에 적재하고, 중앙에서 로그 검색·분석을 합니다. RDB는 트랜잭션, ES는 검색·로그로 **역할을 나눠서** 데이터 특성에 맞게 저장소를 선택한 구조입니다.

---

### Q10. 전시에서 Oracle, MongoDB, Elasticsearch를 같이 쓰는 이유는?

**폴리글랏 퍼시스턴스**를 쓰고 있습니다. **Oracle**은 전시·코너·플랜샵 같은 **트랜잭션·마스터 데이터**용이고, **MongoDB**는 설정·비정형·캐시성 데이터(예: devdispdb)로 RabbitMQ·Config와 맞춰 이벤트/설정 저장에 씁니다. **Elasticsearch**는 검색·로그 전용입니다. 데이터 특성에 맞는 저장소를 쓰면 각각 성능과 운영이 쉬워지고, RDB에 다 넣었을 때의 부하와 스키마 복잡도를 피할 수 있습니다.

---

### Q11. 전시 도메인에서 아쉬운 점이나 개선하고 싶은 점은?

세 가지 정도 있습니다. 첫째, **서비스 수가 많아서**(코너, 플랜샵, 서비스샵, 브랜드스토어, consumer 등) 배포·의존성 관리 비용이 큽니다. 트래픽이 작은 서비스는 **‘전시 코어’ 하나로 묶는 것**을 검토해 볼 만합니다. 둘째, **캐시 무효화**가 명확하지 않으면 코너·카테고리 변경 시 스테일 데이터가 나올 수 있어서, TTL이나 이벤트 기반 무효화 전략을 정리하면 좋겠습니다. 셋째, **라우팅·게이트웨이**가 전시에서 어떻게 되는지(Cloud API /dp/* 매핑, Istio 사용 여부)를 문서로 남기면 온보딩과 장애 대응이 수월할 것 같습니다.

---

## 4. 기술 선택·트레이드오프

### Q12. Kafka와 RabbitMQ를 둘 다 쓰는 이유는 무엇인가요?

**역할을 나눠서** 씁니다. **Kafka**는 배치 결과 적재, 코드/설정 동기화, 로그·이벤트 스트림처럼 **대량·로그성·재생**이 필요한 흐름에 쓰고, **RabbitMQ**는 Spring Cloud Config 연동, 설정 푸시, 작업 큐처럼 **요청/응답·작업 단위** 처리에 가깝게 씁니다. Kafka는 파티션·오프셋으로 스케일과 재처리가 좋고, RabbitMQ는 큐·라우팅으로 작업 분산과 Config 전파에 맞아서, 용도에 맞게 선택한 구조입니다.

---

### Q13. Redis에서 write/read 포트를 분리한 이유는?

회원 LGC에서 **쓰기 1대, 읽기 다대**로 나눠서 읽기 부하를 분산하려고 분리했습니다. 쓰기는 한 곳에서만 하면 일관성이 유지되고, 읽기는 여러 레플리카로 나눠서 **조회 부하**를 줄입니다. Prod에서는 Redis 클러스터(redis.member.ssgadm.com)를 쓰고, 환경에 따라 standalone과 cluster를 나눠서 비용과 가용성을 맞췄습니다.

---

### Q14. Istio를 도입한 이유와 회로 차단이 어떻게 동작하는지 설명해 주세요.

MSA를 K8s에 올렸을 때 **L7 라우팅**과 **트래픽 제어**를 위해 Istio를 씁니다. Gateway와 VirtualService로 호스트·URI 기준으로 서비스를 나누고, **DestinationRule**의 **outlierDetection**으로 특정 파드가 연속 실패하면 일정 시간 동안 트래픽에서 제외합니다. 그래서 한 서비스·인스턴스 장애가 **다른 서비스로 과도하게 전파**되지 않고, 장애 구간만 격리되는 **회로 차단** 효과를 냅니다. Prometheus·Sentry와 함께 쓰면 ‘어디서 실패했는지’와 ‘실패한 인스턴스를 뺐는지’를 같이 볼 수 있어서 복원력과 원인 추적이 가능합니다.

---

## 5. 문제 해결·개선 경험

### Q15. 기존 시스템의 한계를 개선한 경험이 있다면 구체적으로 말해 주세요.

(실제 한 경험이 있다면 그걸 말하고,) 없다면 설계 관점으로 말할 수 있습니다.) 회원은 레거시와 MSA가 공존하는데, **라우팅 규칙**이 코드·설정에만 있어서 새로 합류한 분이 파악하기 어려웠습니다. 그래서 **member/* 가 LGC vs MSA로 어떻게 나뉘는지**를 문서나 라우팅 표로 정리하는 걸 제안했고, (또는 실제로 정리했고,) 온보딩과 장애 대응이 빨라졌습니다. ‘레거시와 MSA 공존’을 장점으로 쓰려면 **라우팅이 명시적으로 정의**되어 있어야 한다는 걸 배웠습니다.

---

### Q16. 장애 대응이나 모니터링은 어떻게 하시나요?

**메트릭·로그·에러**를 한 세트로 씁니다. **Prometheus + Pushgateway**로 LGC·MSA 메트릭을 푸시해서 수집하고, **Sentry**로 예외 스택과 컨텍스트를 모읍니다. **Logback**으로 앱 로그를 남기고, 전시는 **logback-elasticsearch-appender**로 ES에 넣어서 중앙 로그 검색을 합니다. Istio로 **어느 파드가 연속 실패했는지** 트래픽에서 제외되는지 보면서, Prometheus·Sentry로 **원인**을 추적하는 식으로, ‘격리’와 ‘원인 분석’을 같이 보는 구조입니다.

---

### Q17. 다시 설계한다면 바꾸고 싶은 점이 있나요?

회원 쪽이라면 **세 가지**를 바꾸고 싶습니다. 첫째, FO 세션을 처음부터 **Redis**에 두고 웹앱을 무상태로 설계해서 수평 확장을 쉽게 하고, 둘째, 배치는 **한 플랫폼**(예: MMBR-MSA Spring Batch만)으로 모아서 Job·DB 접근을 한 곳에서 관리하고, 셋째, **API 라우팅·버전**을 문서나 설정으로 명시해서 LGC/MSA 전환 시 어떤 요청이 어디로 가는지 항상 추적 가능하게 하겠습니다. 전시 쪽이라면 **캐시 무효화 전략**과 **서비스 그룹화**(필요한 만큼만 서비스 분리)를 설계 단계에서 정리하겠습니다.

---

## 6. 짧게 대답할 때 (30초~1분)

| 질문 | 한 줄 답변 |
|------|-----------------|
| 회원 아키텍처 한 줄로? | 레거시와 MSA를 같이 쓰는 하이브리드이고, BFF가 둘 다 라우팅합니다. |
| CQRS 쓴 이유? | 조회와 변경을 나눠서 조회 서비스만 스케일하고, 읽기/쓰기 모델을 각각 최적화하려고요. |
| 배치 분리한 이유? | 대량 작업이 온라인 서버 리소스를 쓰지 않도록, 전용 배치 앱에서만 돌리려고요. |
| 전시와 회원 차이? | 전시는 MSA만 쓰고 /dp/api로 진입하고, 회원은 LGC+MSA에 /member/* 로 진입합니다. |
| ES 쓰는 이유(전시)? | 검색·로그 전용으로 써서 RDB 부하를 줄이고, 카테고리 검색과 로그 분석을 합니다. |
| Kafka vs RabbitMQ? | Kafka는 대량·이벤트 스트림, RabbitMQ는 Config·작업 큐용으로 나눠 씁니다. |

---

작성일: 2025-02-27  
참고: MMBR-INTERVIEW-SUMMARY.md, DP-INTERVIEW-SUMMARY.md, SYSTEM-TECH-STACK-SUMMARY.md
